W1001 02:25:42.843000 140292368140096 torch/distributed/run.py:779] 
W1001 02:25:42.843000 140292368140096 torch/distributed/run.py:779] *****************************************
W1001 02:25:42.843000 140292368140096 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1001 02:25:42.843000 140292368140096 torch/distributed/run.py:779] *****************************************
[W1001 02:25:47.507831400 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1001 02:25:47.525333382 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1001 02:25:47.551381961 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1001 02:25:47.558411013 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
10/01/2024 02:25:47 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
10/01/2024 02:25:47 - INFO - __main__ -   Training/evaluation parameters RetrieverTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=True,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fix_position_embedding=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
in_batch_negatives=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/xbox/reclm_emb_xbox_bge-m3_v2/runs/Oct01_02-25-47_node-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
negatives_cross_device=False,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/xbox/reclm_emb_xbox_bge-m3_v2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=reclm_emb_xbox_bge-m3_v2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
temperature=0.01,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
)
10/01/2024 02:25:47 - INFO - __main__ -   Model parameters ModelArguments(model_name_or_path='BAAI/bge-m3', config_name=None, tokenizer_name=None, cache_dir=None, sentence_pooling_method='mean', normlized=True, peft_model_name=None, attn_implementation='eager', torch_dtype=None)
10/01/2024 02:25:47 - INFO - __main__ -   Data parameters DataArguments(train_data='data/xbox/train/qwen72B_data_v2.jsonl,data/xbox/train/misspell2item.jsonl,data/xbox/train/negquery2item.jsonl,data/xbox/train/relativequery2item.jsonl,data/xbox/train/title2item.jsonl,data/xbox/train/qwen72B_data.jsonl,data/xbox/train/item2item.jsonl,data/xbox/train/query2item.jsonl,data/xbox/train/queryuser2item.jsonl,data/xbox/train/user2item.jsonl', data_cache_dir='/home/aiscuser/.cache/hf_data', train_group_size=4, query_max_len=512, passage_max_len=128, max_example_num_per_dataset=100000000, has_template=True)
10/01/2024 02:25:47 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
10/01/2024 02:25:47 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
10/01/2024 02:25:47 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
10/01/2024 02:25:48 - INFO - __main__ -   Config: XLMRobertaConfig {
  "_name_or_path": "BAAI/bge-m3",
  "architectures": [
    "XLMRobertaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 8194,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.45.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 768 examples [00:00, 46832.39 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 8460 examples [00:00, 57170.73 examples/s]Generating train split: 19737 examples [00:00, 60185.34 examples/s]Generating train split: 21364 examples [00:00, 59603.37 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 38 examples [00:00, 17750.70 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 150 examples [00:00, 34407.74 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 893 examples [00:00, 47834.20 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1152 examples [00:00, 59808.86 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 68133.26 examples/s]Generating train split: 10000 examples [00:00, 67972.28 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 11996 examples [00:00, 78785.88 examples/s]Generating train split: 18445 examples [00:00, 79328.10 examples/s]
[2024-10-01 02:25:51,245] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-01 02:25:51,272] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-10-01 02:25:51,349] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-10-01 02:25:51,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/home/aiscuser/.local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
node-0:32246:32246 [0] NCCL INFO Bootstrap : Using eth0:10.3.36.181<0>
node-0:32246:32246 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
node-0:32246:32246 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
node-0:32246:32246 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
node-0:32246:32246 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
node-0:32246:32246 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
node-0:32248:32248 [2] NCCL INFO cudaDriverVersion 12020
node-0:32248:32248 [2] NCCL INFO Bootstrap : Using eth0:10.3.36.181<0>
node-0:32249:32249 [3] NCCL INFO cudaDriverVersion 12020
node-0:32249:32249 [3] NCCL INFO Bootstrap : Using eth0:10.3.36.181<0>
node-0:32247:32247 [1] NCCL INFO cudaDriverVersion 12020
node-0:32247:32247 [1] NCCL INFO Bootstrap : Using eth0:10.3.36.181<0>
node-0:32249:32249 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
node-0:32249:32249 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
node-0:32249:32249 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
node-0:32249:32249 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
node-0:32247:32247 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
node-0:32247:32247 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
node-0:32247:32247 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
node-0:32247:32247 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
node-0:32248:32248 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
node-0:32248:32248 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
node-0:32248:32248 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
node-0:32248:32248 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
node-0:32246:32515 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
node-0:32246:32515 [0] NCCL INFO P2P plugin IBext

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32246:32515 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32246:32515 [0] NCCL INFO NET/IB : No device found.

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32246:32515 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32246:32515 [0] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32246:32515 [0] NCCL INFO NET/IB : No device found.
node-0:32246:32515 [0] NCCL INFO NET/Socket : Using [0]eth0:10.3.36.181<0>
node-0:32246:32515 [0] NCCL INFO Using non-device net plugin version 0
node-0:32246:32515 [0] NCCL INFO Using network Socket
node-0:32249:32517 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
node-0:32249:32517 [3] NCCL INFO P2P plugin IBext

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32249:32517 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32249:32517 [3] NCCL INFO NET/IB : No device found.

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32249:32517 [3] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32249:32517 [3] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32249:32517 [3] NCCL INFO NET/IB : No device found.
node-0:32249:32517 [3] NCCL INFO NET/Socket : Using [0]eth0:10.3.36.181<0>
node-0:32249:32517 [3] NCCL INFO Using non-device net plugin version 0
node-0:32249:32517 [3] NCCL INFO Using network Socket
node-0:32247:32519 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
node-0:32247:32519 [1] NCCL INFO P2P plugin IBext

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32247:32519 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32247:32519 [1] NCCL INFO NET/IB : No device found.

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32247:32519 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32247:32519 [1] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32247:32519 [1] NCCL INFO NET/IB : No device found.
node-0:32247:32519 [1] NCCL INFO NET/Socket : Using [0]eth0:10.3.36.181<0>
node-0:32247:32519 [1] NCCL INFO Using non-device net plugin version 0
node-0:32247:32519 [1] NCCL INFO Using network Socket
node-0:32248:32521 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
node-0:32248:32521 [2] NCCL INFO P2P plugin IBext

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32248:32521 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32248:32521 [2] NCCL INFO NET/IB : No device found.

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_0

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_1

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_2

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_3

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_4

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_5

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_6

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_7

node-0:32248:32521 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed

node-0:32248:32521 [2] transport/net_ib.cc:219 NCCL WARN NET/IB : Unable to open device mlx5_8
node-0:32248:32521 [2] NCCL INFO NET/IB : No device found.
node-0:32248:32521 [2] NCCL INFO NET/Socket : Using [0]eth0:10.3.36.181<0>
node-0:32248:32521 [2] NCCL INFO Using non-device net plugin version 0
node-0:32248:32521 [2] NCCL INFO Using network Socket
node-0:32246:32515 [0] NCCL INFO comm 0x5585905f29f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 100000 commId 0xb99c721d7c0ba0f3 - Init START
node-0:32248:32521 [2] NCCL INFO comm 0x559610d31bd0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 300000 commId 0xb99c721d7c0ba0f3 - Init START
node-0:32247:32519 [1] NCCL INFO comm 0x564b2d88a260 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 200000 commId 0xb99c721d7c0ba0f3 - Init START
node-0:32249:32517 [3] NCCL INFO comm 0x55dab69b9510 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 800000 commId 0xb99c721d7c0ba0f3 - Init START
node-0:32248:32521 [2] NCCL INFO Loading topology file /opt/microsoft/ndv5-topo.xml
node-0:32246:32515 [0] NCCL INFO Loading topology file /opt/microsoft/ndv5-topo.xml
node-0:32249:32517 [3] NCCL INFO Loading topology file /opt/microsoft/ndv5-topo.xml
node-0:32247:32519 [1] NCCL INFO Loading topology file /opt/microsoft/ndv5-topo.xml
node-0:32248:32521 [2] NCCL INFO Loading unnamed topology
node-0:32246:32515 [0] NCCL INFO Loading unnamed topology
node-0:32249:32517 [3] NCCL INFO Loading unnamed topology
node-0:32247:32519 [1] NCCL INFO Loading unnamed topology
node-0:32246:32515 [0] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:01.0
node-0:32248:32521 [2] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:01.0
node-0:32247:32519 [1] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:01.0
node-0:32249:32517 [3] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:01.0
node-0:32246:32515 [0] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:02.0
node-0:32248:32521 [2] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:02.0
node-0:32247:32519 [1] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:02.0
node-0:32249:32517 [3] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:02.0
node-0:32246:32515 [0] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:03.0
node-0:32248:32521 [2] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:03.0
node-0:32247:32519 [1] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:03.0
node-0:32249:32517 [3] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:03.0
node-0:32246:32515 [0] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:04.0
node-0:32248:32521 [2] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:04.0
node-0:32247:32519 [1] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:04.0
node-0:32249:32517 [3] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:04.0
node-0:32246:32515 [0] NCCL INFO === System : maxBw 370.8 totalBw 370.8 ===
node-0:32246:32515 [0] NCCL INFO CPU/0 (1/1/2)
node-0:32246:32515 [0] NCCL INFO + PCI[5000.0] - NIC/0
node-0:32246:32515 [0] NCCL INFO + PCI[48.0] - PCI/FFFFFF010 (0)
node-0:32246:32515 [0] NCCL INFO               + PCI[48.0] - GPU/100000 (0)
node-0:32246:32515 [0] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32246:32515 [0] NCCL INFO + PCI[48.0] - PCI/FFFFFF020 (0)
node-0:32246:32515 [0] NCCL INFO               + PCI[48.0] - GPU/200000 (1)
node-0:32246:32515 [0] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32246:32515 [0] NCCL INFO + PCI[48.0] - PCI/FFFFFF030 (0)
node-0:32246:32515 [0] NCCL INFO               + PCI[48.0] - GPU/300000 (2)
node-0:32246:32515 [0] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32246:32515 [0] NCCL INFO + PCI[48.0] - PCI/FFFFFF040 (0)
node-0:32246:32515 [0] NCCL INFO               + PCI[48.0] - GPU/800000 (3)
node-0:32246:32515 [0] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32246:32515 [0] NCCL INFO ==========================================
node-0:32246:32515 [0] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32246:32515 [0] NCCL INFO GPU/200000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32246:32515 [0] NCCL INFO GPU/300000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (0/5000.000000/LOC) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32246:32515 [0] NCCL INFO GPU/800000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (0/5000.000000/LOC) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32246:32515 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffff0000,00000000
node-0:32246:32515 [0] NCCL INFO NVLS multicast support is available on dev 0
node-0:32248:32521 [2] NCCL INFO === System : maxBw 370.8 totalBw 370.8 ===
node-0:32248:32521 [2] NCCL INFO CPU/0 (1/1/2)
node-0:32248:32521 [2] NCCL INFO + PCI[5000.0] - NIC/0
node-0:32248:32521 [2] NCCL INFO + PCI[48.0] - PCI/FFFFFF010 (0)
node-0:32248:32521 [2] NCCL INFO               + PCI[48.0] - GPU/100000 (0)
node-0:32248:32521 [2] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32248:32521 [2] NCCL INFO + PCI[48.0] - PCI/FFFFFF020 (0)
node-0:32248:32521 [2] NCCL INFO               + PCI[48.0] - GPU/200000 (1)
node-0:32248:32521 [2] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32248:32521 [2] NCCL INFO + PCI[48.0] - PCI/FFFFFF030 (0)
node-0:32248:32521 [2] NCCL INFO               + PCI[48.0] - GPU/300000 (2)
node-0:32248:32521 [2] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32248:32521 [2] NCCL INFO + PCI[48.0] - PCI/FFFFFF040 (0)
node-0:32248:32521 [2] NCCL INFO               + PCI[48.0] - GPU/800000 (3)
node-0:32248:32521 [2] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32248:32521 [2] NCCL INFO ==========================================
node-0:32248:32521 [2] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32248:32521 [2] NCCL INFO GPU/200000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32248:32521 [2] NCCL INFO GPU/300000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (0/5000.000000/LOC) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32248:32521 [2] NCCL INFO GPU/800000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (0/5000.000000/LOC) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32248:32521 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,ffff0000,00000000
node-0:32248:32521 [2] NCCL INFO NVLS multicast support is available on dev 2
node-0:32247:32519 [1] NCCL INFO === System : maxBw 370.8 totalBw 370.8 ===
node-0:32247:32519 [1] NCCL INFO CPU/0 (1/1/2)
node-0:32247:32519 [1] NCCL INFO + PCI[5000.0] - NIC/0
node-0:32247:32519 [1] NCCL INFO + PCI[48.0] - PCI/FFFFFF010 (0)
node-0:32247:32519 [1] NCCL INFO               + PCI[48.0] - GPU/100000 (0)
node-0:32247:32519 [1] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32247:32519 [1] NCCL INFO + PCI[48.0] - PCI/FFFFFF020 (0)
node-0:32247:32519 [1] NCCL INFO               + PCI[48.0] - GPU/200000 (1)
node-0:32247:32519 [1] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32247:32519 [1] NCCL INFO + PCI[48.0] - PCI/FFFFFF030 (0)
node-0:32247:32519 [1] NCCL INFO               + PCI[48.0] - GPU/300000 (2)
node-0:32247:32519 [1] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32247:32519 [1] NCCL INFO + PCI[48.0] - PCI/FFFFFF040 (0)
node-0:32247:32519 [1] NCCL INFO               + PCI[48.0] - GPU/800000 (3)
node-0:32247:32519 [1] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32247:32519 [1] NCCL INFO ==========================================
node-0:32247:32519 [1] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32247:32519 [1] NCCL INFO GPU/200000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32247:32519 [1] NCCL INFO GPU/300000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (0/5000.000000/LOC) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32247:32519 [1] NCCL INFO GPU/800000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (0/5000.000000/LOC) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32247:32519 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffff0000,00000000
node-0:32247:32519 [1] NCCL INFO NVLS multicast support is available on dev 1
node-0:32249:32517 [3] NCCL INFO === System : maxBw 370.8 totalBw 370.8 ===
node-0:32249:32517 [3] NCCL INFO CPU/0 (1/1/2)
node-0:32249:32517 [3] NCCL INFO + PCI[5000.0] - NIC/0
node-0:32249:32517 [3] NCCL INFO + PCI[48.0] - PCI/FFFFFF010 (0)
node-0:32249:32517 [3] NCCL INFO               + PCI[48.0] - GPU/100000 (0)
node-0:32249:32517 [3] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32249:32517 [3] NCCL INFO + PCI[48.0] - PCI/FFFFFF020 (0)
node-0:32249:32517 [3] NCCL INFO               + PCI[48.0] - GPU/200000 (1)
node-0:32249:32517 [3] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32249:32517 [3] NCCL INFO + PCI[48.0] - PCI/FFFFFF030 (0)
node-0:32249:32517 [3] NCCL INFO               + PCI[48.0] - GPU/300000 (2)
node-0:32249:32517 [3] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32249:32517 [3] NCCL INFO + PCI[48.0] - PCI/FFFFFF040 (0)
node-0:32249:32517 [3] NCCL INFO               + PCI[48.0] - GPU/800000 (3)
node-0:32249:32517 [3] NCCL INFO                             + NVL[370.8] - NVS/0
node-0:32249:32517 [3] NCCL INFO ==========================================
node-0:32249:32517 [3] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32249:32517 [3] NCCL INFO GPU/200000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (2/370.799988/NVL) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32249:32517 [3] NCCL INFO GPU/300000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (0/5000.000000/LOC) GPU/800000 (2/370.799988/NVL) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32249:32517 [3] NCCL INFO GPU/800000 :GPU/100000 (2/370.799988/NVL) GPU/200000 (2/370.799988/NVL) GPU/300000 (2/370.799988/NVL) GPU/800000 (0/5000.000000/LOC) NVS/0 (1/370.799988/NVL) CPU/0 (2/48.000000/PHB) 
node-0:32249:32517 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffff0000,00000000
node-0:32249:32517 [3] NCCL INFO NVLS multicast support is available on dev 3
node-0:32246:32515 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32246:32515 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32248:32521 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32247:32519 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32249:32517 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32246:32515 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32246:32515 [0] NCCL INFO Pattern 5, crossNic 0, nChannels 4, bw 60.000000/60.000000, type NVL/PIX, sameChannels 0
node-0:32246:32515 [0] NCCL INFO  0 : GPU/0 GPU/0 GPU/0 GPU/0
node-0:32246:32515 [0] NCCL INFO  1 : GPU/1 GPU/0 GPU/0 GPU/0
node-0:32246:32515 [0] NCCL INFO  2 : GPU/2 GPU/0 GPU/0 GPU/0
node-0:32246:32515 [0] NCCL INFO  3 : GPU/3 GPU/0 GPU/0 GPU/0
node-0:32248:32521 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32248:32521 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32248:32521 [2] NCCL INFO Pattern 5, crossNic 0, nChannels 4, bw 60.000000/60.000000, type NVL/PIX, sameChannels 0
node-0:32248:32521 [2] NCCL INFO  0 : GPU/0 GPU/0 GPU/0 GPU/0
node-0:32248:32521 [2] NCCL INFO  1 : GPU/1 GPU/0 GPU/0 GPU/0
node-0:32248:32521 [2] NCCL INFO  2 : GPU/2 GPU/0 GPU/0 GPU/0
node-0:32248:32521 [2] NCCL INFO  3 : GPU/3 GPU/0 GPU/0 GPU/0
node-0:32247:32519 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32247:32519 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32247:32519 [1] NCCL INFO Pattern 5, crossNic 0, nChannels 4, bw 60.000000/60.000000, type NVL/PIX, sameChannels 0
node-0:32247:32519 [1] NCCL INFO  0 : GPU/0 GPU/0 GPU/0 GPU/0
node-0:32247:32519 [1] NCCL INFO  1 : GPU/1 GPU/0 GPU/0 GPU/0
node-0:32247:32519 [1] NCCL INFO  2 : GPU/2 GPU/0 GPU/0 GPU/0
node-0:32247:32519 [1] NCCL INFO  3 : GPU/3 GPU/0 GPU/0 GPU/0
node-0:32249:32517 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 30.000000/30.000000, type NVL/PIX, sameChannels 1
node-0:32249:32517 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  1 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  2 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  3 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  4 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  5 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  6 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  7 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  8 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO  9 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO 10 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO 11 : GPU/0 GPU/1 GPU/2 GPU/3
node-0:32249:32517 [3] NCCL INFO Pattern 5, crossNic 0, nChannels 4, bw 60.000000/60.000000, type NVL/PIX, sameChannels 0
node-0:32249:32517 [3] NCCL INFO  0 : GPU/0 GPU/0 GPU/0 GPU/0
node-0:32249:32517 [3] NCCL INFO  1 : GPU/1 GPU/0 GPU/0 GPU/0
node-0:32249:32517 [3] NCCL INFO  2 : GPU/2 GPU/0 GPU/0 GPU/0
node-0:32249:32517 [3] NCCL INFO  3 : GPU/3 GPU/0 GPU/0 GPU/0
node-0:32248:32521 [2] NCCL INFO comm 0x559610d31bd0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
node-0:32246:32515 [0] NCCL INFO comm 0x5585905f29f0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
node-0:32249:32517 [3] NCCL INFO comm 0x55dab69b9510 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
node-0:32247:32519 [1] NCCL INFO comm 0x564b2d88a260 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
node-0:32246:32515 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
node-0:32248:32521 [2] NCCL INFO Ring 00 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 12 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 00 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 01 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 12 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 01 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 02 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 13 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 02 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 03 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 13 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 2 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 03 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 04 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 2 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 14 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 04 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 05 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 14 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 3 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 05 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 06 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 3 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 15 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 06 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 07 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 15 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 4 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 07 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 08 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 4 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 16 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 08 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 09 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 16 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 5 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 09 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 10 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 5 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 17 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 10 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 11 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 17 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 6 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 11 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 12 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 6 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 18 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 12 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 13 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 18 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 7 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 13 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 14 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 7 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 19 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 14 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 15 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 19 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 8 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 15 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 16 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 8 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 20 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 16 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 17 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 20 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 9 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 17 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 18 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 9 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 21 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 18 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 19 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 21 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 10 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 19 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 20 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 10 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 22 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 20 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 21 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 22 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 11 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 21 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 22 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 11 : 0 -> 1 -> 2/-1/-1
node-0:32246:32515 [0] NCCL INFO Tree 23 : -1 -> 0 -> 1/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 22 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Ring 23 : 1 -> 2 -> 3
node-0:32247:32519 [1] NCCL INFO Tree 23 : 0 -> 1 -> 2/-1/-1
node-0:32249:32517 [3] NCCL INFO Ring 23 : 2 -> 3 -> 0
node-0:32248:32521 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
node-0:32246:32515 [0] NCCL INFO Channel 00/24 :    0   1   2   3
node-0:32249:32517 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
node-0:32247:32519 [1] NCCL INFO Ring 00 : 0 -> 1 -> 2
node-0:32248:32521 [2] NCCL INFO P2P Chunksize set to 524288
node-0:32246:32515 [0] NCCL INFO Channel 01/24 :    0   1   2   3
node-0:32249:32517 [3] NCCL INFO P2P Chunksize set to 524288
node-0:32247:32519 [1] NCCL INFO Ring 01 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 02/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 02 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 03/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 03 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 04/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 04 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 05/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 05 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 06/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 06 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 07/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 07 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 08/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 08 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 09/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 09 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 10/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 10 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 11/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 11 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 12/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 12 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 13/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 13 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 14/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 14 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 15/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 15 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 16/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 16 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 17/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 17 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 18/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 18 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 19/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 19 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 20/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 20 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 21/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 21 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 22/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 22 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Channel 23/24 :    0   1   2   3
node-0:32247:32519 [1] NCCL INFO Ring 23 : 0 -> 1 -> 2
node-0:32246:32515 [0] NCCL INFO Ring 00 : 3 -> 0 -> 1
node-0:32247:32519 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
node-0:32246:32515 [0] NCCL INFO Ring 01 : 3 -> 0 -> 1
node-0:32247:32519 [1] NCCL INFO P2P Chunksize set to 524288
node-0:32246:32515 [0] NCCL INFO Ring 02 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 03 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 04 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 05 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 06 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 07 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 08 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 09 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 10 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 11 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 12 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 13 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 14 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 15 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 16 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 17 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 18 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 19 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 20 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 21 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 22 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Ring 23 : 3 -> 0 -> 1
node-0:32246:32515 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
node-0:32246:32515 [0] NCCL INFO P2P Chunksize set to 524288
node-0:32248:32521 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Connected all rings
node-0:32247:32519 [1] NCCL INFO Connected all rings
node-0:32249:32517 [3] NCCL INFO Connected all rings
node-0:32249:32517 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Connected all rings
node-0:32249:32517 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32249:32517 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32248:32521 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32247:32519 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM
node-0:32246:32515 [0] NCCL INFO Connected all trees
node-0:32247:32519 [1] NCCL INFO Connected all trees
node-0:32249:32517 [3] NCCL INFO Connected all trees
node-0:32248:32521 [2] NCCL INFO Connected all trees
node-0:32246:32515 [0] NCCL INFO NVLS comm 0x5585905f29f0 headRank 0 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 301989888 nvlsTotalSize 1207959552
node-0:32248:32521 [2] NCCL INFO NVLS comm 0x559610d31bd0 headRank 2 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 301989888 nvlsTotalSize 1207959552
node-0:32249:32517 [3] NCCL INFO NVLS comm 0x55dab69b9510 headRank 3 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 301989888 nvlsTotalSize 1207959552
node-0:32247:32519 [1] NCCL INFO NVLS comm 0x564b2d88a260 headRank 1 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 301989888 nvlsTotalSize 1207959552
node-0:32248:32521 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
node-0:32248:32521 [2] NCCL INFO 24 coll channels, 0 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
node-0:32247:32519 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
node-0:32247:32519 [1] NCCL INFO 24 coll channels, 0 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
node-0:32246:32515 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
node-0:32246:32515 [0] NCCL INFO 24 coll channels, 0 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
node-0:32249:32517 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
node-0:32249:32517 [3] NCCL INFO 24 coll channels, 0 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
node-0:32246:32515 [0] NCCL INFO comm 0x5585905f29f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 100000 commId 0xb99c721d7c0ba0f3 - Init COMPLETE
node-0:32248:32521 [2] NCCL INFO comm 0x559610d31bd0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 300000 commId 0xb99c721d7c0ba0f3 - Init COMPLETE
node-0:32249:32517 [3] NCCL INFO comm 0x55dab69b9510 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 800000 commId 0xb99c721d7c0ba0f3 - Init COMPLETE
node-0:32247:32519 [1] NCCL INFO comm 0x564b2d88a260 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 200000 commId 0xb99c721d7c0ba0f3 - Init COMPLETE
[rank3]:[W1001 02:25:55.290475502 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1001 02:25:55.291526401 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1001 02:25:55.292107858 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1001 02:25:55.292557887 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yinita. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.2
wandb: Run data is saved locally in /home/aiscuser/.cache/wandb/run-20241001_022555-ch96fqrq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reclm_emb_xbox_bge-m3_v2
wandb: ⭐️ View project at https://wandb.ai/yinita/RecLM-emb
wandb: 🚀 View run at https://wandb.ai/yinita/RecLM-emb/runs/ch96fqrq
  0%|          | 0/5166 [00:00<?, ?it/s]  0%|          | 1/5166 [00:01<1:54:49,  1.33s/it]  0%|          | 2/5166 [00:02<1:22:53,  1.04it/s]  0%|          | 3/5166 [00:02<1:13:04,  1.18it/s]  0%|          | 4/5166 [00:03<1:08:37,  1.25it/s]  0%|          | 5/5166 [00:04<1:06:01,  1.30it/s]  0%|          | 6/5166 [00:04<1:04:24,  1.34it/s]  0%|          | 7/5166 [00:05<1:03:36,  1.35it/s]  0%|          | 8/5166 [00:06<1:02:58,  1.37it/s]  0%|          | 9/5166 [00:07<1:02:33,  1.37it/s]  0%|          | 10/5166 [00:07<1:02:10,  1.38it/s]  0%|          | 11/5166 [00:08<1:01:50,  1.39it/s]  0%|          | 12/5166 [00:09<1:01:41,  1.39it/s]  0%|          | 13/5166 [00:09<1:01:34,  1.39it/s]  0%|          | 14/5166 [00:10<1:01:29,  1.40it/s]W1001 02:26:07.338000 140292368140096 torch/distributed/elastic/agent/server/api.py:688] Received 2 death signal, shutting down workers
W1001 02:26:07.338000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32246 closing signal SIGINT
W1001 02:26:07.338000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32247 closing signal SIGINT
W1001 02:26:07.338000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32248 closing signal SIGINT
W1001 02:26:07.338000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32249 closing signal SIGINT
Traceback (most recent call last):
  File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 135, in <module>
    main()
  File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 126, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/trainer.py", line 34, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1638, in forward
    return self._post_forward(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1622, in _post_forward
    output = _tree_unflatten_with_rref(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 139, in _tree_unflatten_with_rref
    output = tree_unflatten(output, treespec)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/utils/_pytree.py", line 871, in tree_unflatten
    return treespec.unflatten(leaves)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/utils/_pytree.py", line 808, in unflatten
    return unflatten_fn(child_pytrees, self.context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 472, in _model_output_unflatten
    return output_type(**dict(zip(context, values)))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 7, in __init__
  File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 365, in __post_init__
    class_fields = fields(self)
                   ^^^^^^^^^^^^
  File "/home/aiscuser/miniconda3/lib/python3.12/dataclasses.py", line 1278, in fields
    fields = getattr(class_or_instance, _FIELDS)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 135, in <module>
[rank2]:     main()
[rank2]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 126, in main
[rank2]:     trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
[rank2]:     loss = self.compute_loss(model, inputs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/trainer.py", line 34, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:               ^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 119, in forward
[rank2]:     p_reps = self.encode(passage)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 106, in encode
[rank2]:     psg_out = self.model(**features, return_dict=True)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 977, in forward
[rank2]:     encoder_outputs = self.encoder(
[rank2]:                       ^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 632, in forward
[rank2]:     layer_outputs = layer_module(
[rank2]:                     ^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 521, in forward
[rank2]:     self_attention_outputs = self.attention(
[rank2]:                              ^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 448, in forward
[rank2]:     self_outputs = self.self(
[rank2]:                    ^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 274, in forward
[rank2]:     context_layer = torch.matmul(attention_probs, value_layer)
[rank2]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: KeyboardInterrupt
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 135, in <module>
[rank3]:     main()
[rank3]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 126, in main
[rank3]:     trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
[rank3]:     loss = self.compute_loss(model, inputs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/trainer.py", line 34, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:               ^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 119, in forward
[rank3]:     p_reps = self.encode(passage)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 106, in encode
[rank3]:     psg_out = self.model(**features, return_dict=True)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 977, in forward
[rank3]:     encoder_outputs = self.encoder(
[rank3]:                       ^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 632, in forward
[rank3]:     layer_outputs = layer_module(
[rank3]:                     ^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 521, in forward
[rank3]:     self_attention_outputs = self.attention(
[rank3]:                              ^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 448, in forward
[rank3]:     self_outputs = self.self(
[rank3]:                    ^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 217, in forward
[rank3]:     key_layer = self.transpose_for_scores(self.key(hidden_states))
[rank3]:                                           ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 117, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 135, in <module>
[rank0]:     main()
[rank0]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 126, in main
[rank0]:     trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/trainer.py", line 34, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:               ^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1638, in forward
[rank0]:     return self._post_forward(output)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1622, in _post_forward
[rank0]:     output = _tree_unflatten_with_rref(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 139, in _tree_unflatten_with_rref
[rank0]:     output = tree_unflatten(output, treespec)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/utils/_pytree.py", line 871, in tree_unflatten
[rank0]:     return treespec.unflatten(leaves)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/utils/_pytree.py", line 808, in unflatten
[rank0]:     return unflatten_fn(child_pytrees, self.context)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 472, in _model_output_unflatten
[rank0]:     return output_type(**dict(zip(context, values)))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "<string>", line 7, in __init__
[rank0]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 365, in __post_init__
[rank0]:     class_fields = fields(self)
[rank0]:                    ^^^^^^^^^^^^
[rank0]:   File "/home/aiscuser/miniconda3/lib/python3.12/dataclasses.py", line 1278, in fields
[rank0]:     fields = getattr(class_or_instance, _FIELDS)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 135, in <module>
[rank1]:     main()
[rank1]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/train.py", line 126, in main
[rank1]:     trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
[rank1]:     loss = self.compute_loss(model, inputs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/trainer.py", line 34, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:               ^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 119, in forward
[rank1]:     p_reps = self.encode(passage)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/remote_github/yinita/RecAI/RecLM-emb/src/model.py", line 106, in encode
[rank1]:     psg_out = self.model(**features, return_dict=True)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 977, in forward
[rank1]:     encoder_outputs = self.encoder(
[rank1]:                       ^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 632, in forward
[rank1]:     layer_outputs = layer_module(
[rank1]:                     ^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 563, in forward
[rank1]:     layer_output = apply_chunking_to_forward(
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 248, in apply_chunking_to_forward
[rank1]:     return forward_fn(*input_tensors)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 576, in feed_forward_chunk
[rank1]:     layer_output = self.output(intermediate_output, attention_output)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 489, in forward
[rank1]:     hidden_states = self.LayerNorm(hidden_states + input_tensor)
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 202, in forward
[rank1]:     return F.layer_norm(
[rank1]:            ^^^^^^^^^^^^^
[rank1]:   File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 2576, in layer_norm
[rank1]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: KeyboardInterrupt
W1001 02:26:07.493000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32246 closing signal SIGTERM
W1001 02:26:07.493000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32247 closing signal SIGTERM
W1001 02:26:07.494000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32248 closing signal SIGTERM
W1001 02:26:07.494000 140292368140096 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32249 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 32178 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 689, in run
    self._shutdown(e.sigval)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 347, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 544, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 868, in _close
    handler.proc.wait(time_to_wait)
  File "/home/aiscuser/miniconda3/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/miniconda3/lib/python3.12/subprocess.py", line 2045, in _wait
    time.sleep(delay)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 32178 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/aiscuser/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 694, in run
    self._shutdown()
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 347, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 544, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 868, in _close
    handler.proc.wait(time_to_wait)
  File "/home/aiscuser/miniconda3/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aiscuser/miniconda3/lib/python3.12/subprocess.py", line 2045, in _wait
    time.sleep(delay)
  File "/home/aiscuser/.local/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 32178 got signal: 2
